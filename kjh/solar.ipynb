{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "effb10df",
   "metadata": {},
   "source": [
    "# RAG 파이프라인 (Elasticsearch + Sentence‑BERT + Solar_API)\n",
    "이 노트북은 전체 파이프라인 코드를 **모든 줄 주석**과 함께 보여줍니다.\n",
    "각 섹션별로 Markdown 설명 → Code 셀 순서로 구성되어 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74152bf2",
   "metadata": {},
   "source": [
    "## 📦 라이브러리 임포트 및 모델 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "047461a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import requests\n",
    "import traceback\n",
    "import time\n",
    "\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "model = SentenceTransformer(\"snunlp/KR-SBERT-V40K-klueNLI-augSTS\")     # 한국어 SBERT 모델 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b321cf4",
   "metadata": {},
   "source": [
    "### 오류 메시지 안나오게 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52bf537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "\n",
    "# InsecureRequestWarning 끄기\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f120124e",
   "metadata": {},
   "source": [
    "### .env 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98965f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY: 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\n",
      "ES_PASSWORD: EoNb_y3k0ijlW0PlKxnt\n",
      "SOLAR_API_KEY: up_kntyD3xQnR27odlyiYxbsL1c3HsfP\n",
      "SOLAR_API_URL: https://api.upstage.ai/v1/solar/chat/completions\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")\n",
    "ES_PASSWORD=os.getenv(\"ES_PASSWORD\")\n",
    "SOLAR_API_KEY=os.getenv(\"SOLAR_API_KEY\")\n",
    "SOLAR_API_URL=os.getenv(\"SOLAR_API_URL\")\n",
    "\n",
    "print(\"OPENAI_API_KEY:\", OPENAI_API_KEY)\n",
    "print(\"ES_PASSWORD:\", ES_PASSWORD)\n",
    "print(\"SOLAR_API_KEY:\", SOLAR_API_KEY)\n",
    "print(\"SOLAR_API_URL:\", SOLAR_API_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5663585d",
   "metadata": {},
   "source": [
    "## 임베딩 함수\n",
    "1. get_embedding(sentences) 함수\n",
    "- sentences: 텍스트(또는 텍스트 리스트)가 들어오는 입력.\n",
    "- model.encode(sentences): model 객체 (보통 Sentence-BERT 같은 임베딩 모델)을 이용해서 입력 문장들을 벡터로 변환하는 함수.\n",
    "- 결과적으로 문장 하나당 하나의 벡터를 뽑아주는 역할.\n",
    "\n",
    "2. get_embeddings_in_batches(docs, batch_size=100) 함수\n",
    "- docs: 여러 개의 문서를 담은 리스트야. (문서 하나는 딕셔너리 형태, 예: {\"content\": \"내용\"})\n",
    "- batch_size: 한 번에 처리할 문서 수. 기본값은 100개.\n",
    "- 작동 방식:\n",
    "    - 전체 문서를 batch_size만큼 나눠서,\n",
    "    - 각 배치마다 문서의 \"content\"만 뽑아,\n",
    "    - get_embedding()으로 임베딩을 계산하고,\n",
    "    - 결과를 하나의 리스트로 이어붙인다.\n",
    "\n",
    "매 배치마다 진행 상황을 출력해서, \"몇 번째 문서까지 처리했는지\" 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1667cc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(sentences):               # 문장 리스트를 임베딩으로 변환\n",
    "    return model.encode(sentences)           # 모델을 이용해 문장들을 임베딩 벡터로 변환\n",
    "\n",
    "\n",
    "def get_embeddings_in_batches(docs, batch_size=100):  # 배치 단위 임베딩 생성\n",
    "    batch_embeddings = []                              \n",
    "    for i in range(0, len(docs), batch_size):          # 0부터 끝까지 batch_size씩 슬라이싱\n",
    "        batch = docs[i:i + batch_size]                 # 현재 배치 추출\n",
    "        contents = [doc[\"content\"] for doc in batch]   # 문서 본문만 리스트로 추출\n",
    "        embeddings = get_embedding(contents)           # 임베딩 계산\n",
    "        batch_embeddings.extend(embeddings)            # 결과 누적\n",
    "        print(f'batch {i}')                            # 진행 상황 출력\n",
    "    return batch_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1269ae",
   "metadata": {},
   "source": [
    "## ES 인덱스 관리\n",
    "1. create_es_index(index, settings, mappings)\n",
    "- index: 만들고 싶은 인덱스 이름 (예: \"test\")\n",
    "- settings: 아까 만든 분석기 설정 (settings 변수)\n",
    "- mappings: 아까 만든 필드 구조 (mappings 변수)\n",
    "\n",
    "2. delete_es_index(index)\n",
    "- index: 삭제하고 싶은 인덱스 이름.\n",
    "- 이름에 해당하는 인덱스를 삭제\n",
    "\n",
    "3. bulk_add(index, docs)\n",
    "- index: 데이터를 집어넣을 인덱스 이름.\n",
    "- docs: 업로드할 문서들 리스트. (doc는 보통 { \"content\": \"텍스트\", \"embeddings\": [...] } 형태)\n",
    "- 작동 방식:\n",
    "    - 문서 리스트를 Elasticsearch가 이해할 수 있는 bulk 작업용 액션 리스트로 바꾼다.\n",
    "    - helpers.bulk를 사용해서 한 번에 대량 업로드한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "944ab258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_es_index(index, settings, mappings):        # 새 인덱스 생성 함수\n",
    "    if es.indices.exists(index=index):                 # 이미 존재하면\n",
    "        es.indices.delete(index=index)                 # 삭제 후 재생성\n",
    "    es.indices.create(index=index,                    # 인덱스 생성\n",
    "                      settings=settings,\n",
    "                      mappings=mappings)\n",
    "\n",
    "def delete_es_index(index):                           # 인덱스 삭제 래퍼\n",
    "    es.indices.delete(index=index)\n",
    "\n",
    "def bulk_add(index, docs):                            # 대량 색인을 위한 헬퍼\n",
    "    actions = [{'_index': index, '_source': doc} for doc in docs]  # 액션 목록 생성\n",
    "    return helpers.bulk(es, actions)                  # helpers.bulk 로 일괄 업로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00614f63",
   "metadata": {},
   "source": [
    "## 📚 ES 검색 함수 정리\n",
    "\n",
    "📊 요약 표\n",
    "\n",
    "| 함수 이름 | 검색 방식 | 특징 |\n",
    "|:---|:---|:---|\n",
    "| `sparse_retrieve` | BM25 (텍스트 기반 역색인 검색) | 정확한 단어 매칭에 강함 |\n",
    "| `dense_retrieve` | 임베딩 기반 벡터 검색 | 의미 기반 검색에 강함 |\n",
    "| `hybrid_retrieve` | sparse + dense 점수 병합 | 단어 + 의미 모두 고려하여 검색 |\n",
    "### 1. `sparse_retrieve(query_str, size)`\n",
    "\n",
    "- **query_str**: 검색할 키워드(텍스트)\n",
    "- **size**: 가져올 문서 수\n",
    "- **작동 방식**:\n",
    "  - 입력된 `query_str`을 **BM25 방식**으로 `content` 필드에 매칭\n",
    "  - **_score** 기준으로 관련성 높은 문서를 `size`개 반환\n",
    "- **특징**:\n",
    "  - **정확한 키워드 매칭**에 강함\n",
    "  - 의미 기반 매칭은 약함\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `dense_retrieve(query_str, size)`\n",
    "\n",
    "- **query_str**: 검색할 키워드(텍스트)\n",
    "- **size**: 가져올 문서 수\n",
    "- **작동 방식**:\n",
    "  - `query_str`을 **임베딩 벡터**로 변환\n",
    "  - 저장된 문서 벡터들과 **거리(L2 norm)** 비교\n",
    "  - 가장 유사도가 높은 문서 `size`개 반환\n",
    "- **특징**:\n",
    "  - **의미 기반 검색**(비슷한 뜻 찾기)에 강함\n",
    "  - 정확한 키워드 매칭에는 약할 수 있음\n",
    "\n",
    "---\n",
    "\n",
    "### 3. `hybrid_retrieve(query_str, size)`\n",
    "\n",
    "- **query_str**: 검색할 키워드(텍스트)\n",
    "- **size**: 가져올 문서 수\n",
    "- **작동 방식**:\n",
    "  1. `sparse_retrieve`로 **BM25** 기반 검색 결과 가져오기\n",
    "  2. `dense_retrieve`로 **임베딩** 기반 검색 결과 가져오기\n",
    "  3. **문서 ID 기준 병합**\n",
    "  4. **sparse score**와 **dense score**를 **가중 평균**하여 최종 점수 계산\n",
    "  5. 최종 점수 기준으로 상위 `size`개 문서 선택\n",
    "- **특징**:\n",
    "  - **단어 기반 + 의미 기반**을 모두 반영\n",
    "  - 단독 sparse나 dense보다 **더 강력하고 균형 잡힌 검색 결과** 제공\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6f248d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_retrieve(query_str, size):                 # 역색인(BM25) 검색\n",
    "    query = {\"match\": {\"content\": {\"query\": query_str}}}\n",
    "    return es.search(index=\"test\",\n",
    "                     query=query,\n",
    "                     size=size,\n",
    "                     sort=\"_score\")\n",
    "\n",
    "def dense_retrieve(query_str, size):                  # 벡터 KNN 검색\n",
    "    query_embedding = get_embedding([query_str])[0]   # 쿼리 임베딩\n",
    "    knn = {                                           # KNN 파라미터\n",
    "        \"field\": \"embeddings\",\n",
    "        \"query_vector\": query_embedding.tolist(),\n",
    "        \"k\": size,\n",
    "        \"num_candidates\": 100\n",
    "    }\n",
    "    return es.search(index=\"test\", knn=knn)           # ES 8.x KNN 검색 호출\n",
    "\n",
    "\n",
    "def hybrid_retrieve(query_str, size, sparse_weight=0.6, dense_weight=0.4):\n",
    "    \"\"\"\n",
    "    sparse(BM25) + dense(KNN) 결과를 가중 평균하여 결합하는 hybrid retrieval 함수\n",
    "    - sparse_weight: BM25 점수의 비중\n",
    "    - dense_weight: dense 점수의 비중\n",
    "    \"\"\"\n",
    "\n",
    "    # sparse(BM25) 검색 결과 가져오기\n",
    "    sparse_result = sparse_retrieve(query_str, size)\n",
    "\n",
    "    # dense(KNN) 검색 결과 가져오기\n",
    "    dense_result = dense_retrieve(query_str, size)\n",
    "\n",
    "    # 문서 ID를 기준으로 결과를 병합하기 위한 딕셔너리 초기화\n",
    "    merged_hits = {}\n",
    "\n",
    "    # sparse 검색 결과를 먼저 처리\n",
    "    for hit in sparse_result[\"hits\"][\"hits\"]:\n",
    "        doc_id = hit[\"_id\"]\n",
    "        merged_hits[doc_id] = {\n",
    "            \"source\": hit[\"_source\"],\n",
    "            \"sparse_score\": hit[\"_score\"],\n",
    "            \"dense_score\": 0.0\n",
    "        }\n",
    "\n",
    "    # dense 검색 결과를 처리\n",
    "    for hit in dense_result[\"hits\"][\"hits\"]:\n",
    "        doc_id = hit[\"_id\"]\n",
    "        if doc_id in merged_hits:\n",
    "            merged_hits[doc_id][\"dense_score\"] = hit[\"_score\"]\n",
    "        else:\n",
    "            merged_hits[doc_id] = {\n",
    "                \"source\": hit[\"_source\"],\n",
    "                \"sparse_score\": 0.0,\n",
    "                \"dense_score\": hit[\"_score\"]\n",
    "            }\n",
    "\n",
    "    # sparse와 dense 점수를 가중 평균 (비율 조정 가능)\n",
    "    for doc_id in merged_hits:\n",
    "        merged_hits[doc_id][\"final_score\"] = (\n",
    "            sparse_weight * merged_hits[doc_id][\"sparse_score\"]\n",
    "            + dense_weight * merged_hits[doc_id][\"dense_score\"]\n",
    "        )\n",
    "\n",
    "    # 최종 스코어 기준으로 정렬 (높은 점수 순)\n",
    "    ranked_hits = sorted(merged_hits.items(), key=lambda x: x[1][\"final_score\"], reverse=True)\n",
    "\n",
    "    # top size개만 선택\n",
    "    top_hits = ranked_hits[:size]\n",
    "\n",
    "    # Elasticsearch 결과처럼 포맷 정리\n",
    "    results = []\n",
    "    for doc_id, info in top_hits:\n",
    "        results.append({\n",
    "            \"_id\": doc_id,\n",
    "            \"_score\": info[\"final_score\"],\n",
    "            \"_source\": info[\"source\"]\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71148a3d",
   "metadata": {},
   "source": [
    "## ES 클라이언트 설정\n",
    "### 서버 버전과 호환이 안됨. 코드 fix 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b5ffbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'instance-15246', 'cluster_name': 'elasticsearch', 'cluster_uuid': 'ixlY2lorQmS5rzNDpvAYVQ', 'version': {'number': '8.8.0', 'build_flavor': 'default', 'build_type': 'tar', 'build_hash': 'c01029875a091076ed42cdb3a41c10b1a9a5a20f', 'build_date': '2023-05-23T17:16:07.179039820Z', 'build_snapshot': False, 'lucene_version': '9.6.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rag/lib/python3.10/site-packages/elasticsearch/_sync/client/__init__.py:395: SecurityWarning: Connecting to 'https://localhost:9200' using TLS with verify_certs=False is insecure\n",
      "  _transport = transport_class(\n"
     ]
    }
   ],
   "source": [
    "es_username = \"elastic\"                               # ES 기본 사용자\n",
    "es_password = ES_PASSWORD           # ← 실제 비밀번호로 교체\n",
    "\n",
    "es = Elasticsearch(\n",
    "    \"https://localhost:9200\",\n",
    "    basic_auth=(es_username, es_password),\n",
    "    verify_certs=False,        # CA 무시(테스트용)\n",
    ")\n",
    "\n",
    "print(es.info())  # 이제 정상 출력 (예: 7.17.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f33ca1b",
   "metadata": {},
   "source": [
    "## 인덱스 설정 & 매핑\n",
    "Elasticsearch에서 한국어 텍스트 + 임베딩 벡터를 저장할 수 있는 인덱스를 만들기 위한 설정.  \n",
    "\n",
    "1. settings 부분\n",
    "- nori 분석기는 Nori Tokenizer를 기반으로 함.\n",
    "- 조사, 어미, 기호 같은 불필요한 품사 토큰을 제거\n",
    "    - E(어미), J(조사), SC(구두점), SE(문장구분), SF(마침표), VCN(형용사), VCP(긍정지정사), VX(보조동사)\n",
    "- 핵심 던어만 남김.\n",
    "\n",
    "2. mappings 부분\n",
    "- content: 일반 텍스트 필드, 검색할 때 nori 분석기로 전처리 해서 인덱싱.\n",
    "- embeddings: 768 차원 벡터 필드를 저장하고, 벡터 검색을 할 수 있도록 설정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3d544da",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {                                          # 한글용 Nori 분석기 설정\n",
    "    \"analysis\": {\n",
    "        \"analyzer\": {\n",
    "            \"nori\": {                               # 'nori'라는 사용자 정의 분석기를 만든다\n",
    "                \"type\": \"custom\",                   # 직접 필터와 토크나이저를 설정하는 방식\n",
    "                \"tokenizer\": \"nori_tokenizer\",      # 한국어용 기본 Nori 토크나이저 사용\n",
    "                \"decompound_mode\": \"mixed\",         # 복합어는 분리도 하고 원형도 같이 보존(mixed)\n",
    "                \"filter\": [\"nori_posfilter\", \"synonym_filter\"]        # 필터링 적용\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        \"normalizer\": {\n",
    "            \"lowercase_normalizer\": {\n",
    "            \"type\": \"custom\",\n",
    "            \"filter\": [\"lowercase\"]\n",
    "            }\n",
    "        },\n",
    "\n",
    "        \"filter\": {\n",
    "            \"synonym_filter\": {\n",
    "                \"type\": \"synonym\",\n",
    "                # \"synonyms_path\": \"/usr/share/elasticsearch/config/synonym_filter.txt\", # 파일 경로 # 헬륨, helium, HeLiUm\n",
    "                \"synonyms\": [\"운동, 체육, 스포츠\"] # 헬륨, helium, HeLiUm\n",
    "                # \"updateable\": True  #  (7.3 이상) 동적 업데이트\n",
    "            },\n",
    "            \"nori_posfilter\": {  # 사용자 정의 품사 필터\n",
    "                \"type\": \"nori_part_of_speech\",  # 품사 기반 필터링을 수행\n",
    "                \"stoptags\": [\"E\", \"J\", \"SC\", \"SE\", \"SF\", \"VCN\", \"VCP\", \"VX\"]  \n",
    "                # 이 품사에 해당하는 토큰들은 제거한다\n",
    "                # E(어미), J(조사), SC(구두점), SE(문장구분), SF(마침표), VCN(형용사), VCP(긍정지정사), VX(보조동사)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "mappings = {\n",
    "    \"properties\": {\n",
    "        \"content\": {\"type\": \"text\", \"analyzer\": \"nori\"},  # 'content' 필드는 위에서 정의한 nori 분석기로 분석\n",
    "        \"embeddings\": {  \n",
    "            \"type\": \"dense_vector\",  # 'embeddings' 필드는 밀집 벡터(dense vector)로 저장\n",
    "            \"dims\": 768,  # 임베딩 벡터 차원 수는 768 (ex: BERT base 모델 output 차원)\n",
    "            \"index\": True,  # 벡터를 검색(indexing) 가능하게 설정\n",
    "            \"similarity\": \"l2_norm\"  # 벡터 유사도는 L2 노름(유클리드 거리) 기반으로 측정\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "create_es_index(\"test\", settings, mappings)           # ‘test’ 인덱스 생성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4826df3",
   "metadata": {},
   "source": [
    "## 문서 로드 & 색인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b91b1f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "batch 100\n",
      "batch 200\n",
      "batch 300\n",
      "batch 400\n",
      "batch 500\n",
      "batch 600\n",
      "batch 700\n",
      "batch 800\n",
      "batch 900\n",
      "batch 1000\n",
      "batch 1100\n",
      "batch 1200\n",
      "batch 1300\n",
      "batch 1400\n",
      "batch 1500\n",
      "batch 1600\n",
      "batch 1700\n",
      "batch 1800\n",
      "batch 1900\n",
      "batch 2000\n",
      "batch 2100\n",
      "batch 2200\n",
      "batch 2300\n",
      "batch 2400\n",
      "batch 2500\n",
      "batch 2600\n",
      "batch 2700\n",
      "batch 2800\n",
      "batch 2900\n",
      "batch 3000\n",
      "batch 3100\n",
      "batch 3200\n",
      "batch 3300\n",
      "batch 3400\n",
      "batch 3500\n",
      "batch 3600\n",
      "batch 3700\n",
      "batch 3800\n",
      "batch 3900\n",
      "batch 4000\n",
      "batch 4100\n",
      "batch 4200\n",
      "(4272, [])\n"
     ]
    }
   ],
   "source": [
    "index_docs = []                                       # 색인용 문서 리스트\n",
    "with jsonlines.open(\"./data/documents.jsonl\") as reader:\n",
    "    docs = list(reader)                # 각 줄이 dict로 바로 변환됨\n",
    "\n",
    "embeddings = get_embeddings_in_batches(docs)          # 임베딩 배치 생성\n",
    "\n",
    "for doc, emb in zip(docs, embeddings):                # 문서와 임베딩 병합\n",
    "    doc[\"embeddings\"] = emb.tolist()                  # numpy → list 변환\n",
    "    index_docs.append(doc)\n",
    "\n",
    "ret = bulk_add(\"test\", index_docs)                    # ES에 대량 색인\n",
    "print(ret)                                            # 색인 결과 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6460149",
   "metadata": {},
   "source": [
    "## 검색 예시 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80dfc05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse score: 33.710434 source: 금성이 다른 행성들보다 더 밝게 보이는 이유는 지구 쪽으로 가장 많은 햇빛을 반사하기 때문입니다. 케빈은 맑은 밤에 하늘을 관찰하고 있습니다. 그는 맨눈으로 금성, 화성, 목성, 토성을 볼 수 있습니다. 금성은 햇빛을 많이 반사하기 때문에 다른 행성들보다 더 밝게 보입니다. 이는 금성의 표면이 반사율이 높기 때문입니다. 금성은 태양으로부터 받은 햇빛을 표면에 반사하여 지구에서 관찰하기 쉽게 만듭니다. 따라서 케빈은 맑은 밤에 금성을 더 밝게 볼 수 있습니다.\n",
      "sparse score: 18.925915 source: 금성은 태양계의 두 번째로 가까운 행성입니다. 이 행성의 대략적인 나이는 7억 5천만 년으로 추정됩니다. 금성은 지구와 매우 비슷한 크기와 구성을 가지고 있으며, 약 90% 이상이 이산화탄소로 이루어져 있습니다. 이 행성은 매우 뜨거운 온도와 압력을 가지고 있어서 인간이 살 수 있는 환경이 아닙니다. 금성의 대기는 두꺼워서 태양의 열을 가두고 있어서 행성의 표면은 평균 온도가 약 450도로 매우 뜨거운 상태입니다. 또한, 금성은 자전 속도가 매우 빠르기 때문에 하루가 지구의 약 243일과 같습니다. 이러한 특징들로 인해 금성은 우리 태양계에서 가장 가혹한 환경을 가진 행성 중 하나로 알려져 있습니다.\n",
      "sparse score: 18.749123 source: 메릴랜드 Space Grant Observatory는 볼티모어에 위치해 있습니다. 학생들은 이 망원경을 방문하여 별, 행성, 달에 대해 배웠습니다. 그들은 아래와 같은 정보를 기록했습니다. 첫째, 별 패턴은 그대로 유지되지만, 하늘에서의 위치는 변하는 것처럼 보입니다. 둘째, 태양, 행성, 달은 하늘에서 움직이는 것처럼 보입니다. 셋째, 켄타우루스자리의 프록시마 성은 우리 태양계에서 가장 가까운 별입니다. 넷째, 북극성은 소 북두칠성이라 불리는 별 패턴의 일부입니다. 그렇다면 태양이 매일 하늘을 가로질러 움직이는 것처럼 보이는 이유는 무엇일까요? 지구가 자전축을 중심으로 회전하기 때문입니다.\n",
      "dense score: 0.0047387844 source: 금성이 다른 행성들보다 더 밝게 보이는 이유는 지구 쪽으로 가장 많은 햇빛을 반사하기 때문입니다. 케빈은 맑은 밤에 하늘을 관찰하고 있습니다. 그는 맨눈으로 금성, 화성, 목성, 토성을 볼 수 있습니다. 금성은 햇빛을 많이 반사하기 때문에 다른 행성들보다 더 밝게 보입니다. 이는 금성의 표면이 반사율이 높기 때문입니다. 금성은 태양으로부터 받은 햇빛을 표면에 반사하여 지구에서 관찰하기 쉽게 만듭니다. 따라서 케빈은 맑은 밤에 금성을 더 밝게 볼 수 있습니다.\n",
      "dense score: 0.004689847 source: 금성은 태양계에서 가장 가까운 행성 중 하나입니다. 그러나 화성이나 지구처럼 계절이 없는 이유는 금성의 자전축이 태양계의 평면에 거의 수직이기 때문입니다. 자전축이 수직이기 때문에 금성은 태양으로부터 받는 햇빛의 양이 일정하게 유지됩니다. 이로 인해 금성은 계절 변화가 없으며 항상 일정한 온도를 유지합니다. 이러한 환경은 생명체에게는 적합하지 않을 수 있지만, 금성의 특이한 기후 조건은 우주 탐사에 대한 연구에 많은 도움을 주고 있습니다. 금성은 여전히 우리에게 알려지지 않은 많은 비밀을 품고 있으며, 미래에 더 많은 연구와 탐사가 이루어질 것으로 기대됩니다.\n",
      "dense score: 0.0042649126 source: 소행성대가 위치한 곳에는 행성이 없는 이유는 목성과의 공명으로 인해 물질이 모이는 것을 방해하여 행성을 형성하지 못했기 때문입니다. 소행성대는 태양계 내부 행성들과 외부 행성들 사이에 위치한 영역으로, 많은 소행성들이 모여있는 지역입니다. 하지만 목성과의 공명 현상으로 인해 소행성들이 목성의 중력에 영향을 받아 행성을 형성하지 못하고 분산되거나 파괴되는 경우가 많습니다. 따라서 소행성대는 행성이 형성되기 어려운 환경이라고 할 수 있습니다. 이러한 이유로 소행성대에는 행성이 없는 것입니다.\n",
      "hybrid score: 20.22815591376 source: 금성이 다른 행성들보다 더 밝게 보이는 이유는 지구 쪽으로 가장 많은 햇빛을 반사하기 때문입니다. 케빈은 맑은 밤에 하늘을 관찰하고 있습니다. 그는 맨눈으로 금성, 화성, 목성, 토성을 볼 수 있습니다. 금성은 햇빛을 많이 반사하기 때문에 다른 행성들보다 더 밝게 보입니다. 이는 금성의 표면이 반사율이 높기 때문입니다. 금성은 태양으로부터 받은 햇빛을 표면에 반사하여 지구에서 관찰하기 쉽게 만듭니다. 따라서 케빈은 맑은 밤에 금성을 더 밝게 볼 수 있습니다.\n",
      "hybrid score: 11.355549 source: 금성은 태양계의 두 번째로 가까운 행성입니다. 이 행성의 대략적인 나이는 7억 5천만 년으로 추정됩니다. 금성은 지구와 매우 비슷한 크기와 구성을 가지고 있으며, 약 90% 이상이 이산화탄소로 이루어져 있습니다. 이 행성은 매우 뜨거운 온도와 압력을 가지고 있어서 인간이 살 수 있는 환경이 아닙니다. 금성의 대기는 두꺼워서 태양의 열을 가두고 있어서 행성의 표면은 평균 온도가 약 450도로 매우 뜨거운 상태입니다. 또한, 금성은 자전 속도가 매우 빠르기 때문에 하루가 지구의 약 243일과 같습니다. 이러한 특징들로 인해 금성은 우리 태양계에서 가장 가혹한 환경을 가진 행성 중 하나로 알려져 있습니다.\n",
      "hybrid score: 11.2494738 source: 메릴랜드 Space Grant Observatory는 볼티모어에 위치해 있습니다. 학생들은 이 망원경을 방문하여 별, 행성, 달에 대해 배웠습니다. 그들은 아래와 같은 정보를 기록했습니다. 첫째, 별 패턴은 그대로 유지되지만, 하늘에서의 위치는 변하는 것처럼 보입니다. 둘째, 태양, 행성, 달은 하늘에서 움직이는 것처럼 보입니다. 셋째, 켄타우루스자리의 프록시마 성은 우리 태양계에서 가장 가까운 별입니다. 넷째, 북극성은 소 북두칠성이라 불리는 별 패턴의 일부입니다. 그렇다면 태양이 매일 하늘을 가로질러 움직이는 것처럼 보이는 이유는 무엇일까요? 지구가 자전축을 중심으로 회전하기 때문입니다.\n"
     ]
    }
   ],
   "source": [
    "test_query = \"금성이 다른 행성들보다 밝게 보이는 이유는 무엇인가요?\"  # 샘플 쿼리\n",
    "\n",
    "search_result_retrieve = sparse_retrieve(test_query, 3)  # BM25 검색\n",
    "for hit in search_result_retrieve['hits']['hits']:       # 결과 출력\n",
    "    print('sparse score:', hit['_score'],\n",
    "          'source:', hit['_source'][\"content\"])\n",
    "\n",
    "search_result_retrieve = dense_retrieve(test_query, 3)   # 벡터 검색\n",
    "for hit in search_result_retrieve['hits']['hits']:\n",
    "    print('dense score:', hit['_score'],\n",
    "          'source:', hit['_source'][\"content\"])\n",
    "    \n",
    "search_result_retrieve = hybrid_retrieve(test_query, 3)   # 하이브리드 검색\n",
    "for hit in search_result_retrieve:\n",
    "    print('hybrid score:', hit['_score'],\n",
    "          'source:', hit['_source'][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b87ca6",
   "metadata": {},
   "source": [
    "### SOLAR CALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698ff5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_solar(messages, model=\"solar-1-mini-chat\", temperature=0.0, top_p=0.9):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {SOLAR_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"n\": 1\n",
    "    }\n",
    "    \n",
    "    response = requests.post(SOLAR_API_URL, headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()  # ✅ 여기! .text 말고 .json()으로!!\n",
    "    else:\n",
    "        print(f\"Solar API Error {response.status_code}: {response.text}\")\n",
    "        raise Exception(f\"Solar API 호출 실패 (status_code: {response.status_code})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7511fb",
   "metadata": {},
   "source": [
    "## RAG 핵심 로직\n",
    "### open_ai 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912c1d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# persona.yaml 불러오기\n",
    "with open(\"persona.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    persona_config = yaml.safe_load(f)\n",
    "\n",
    "# 변수로 꺼내 쓰기\n",
    "persona_function_calling = persona_config[\"persona_function_calling\"]\n",
    "persona_qa = persona_config[\"persona_qa\"]\n",
    "\n",
    "llm_model = \"solar-pro\"                             # 사용할 LLM 이름\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decadca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(messages):  # 대화 기록(messages)을 입력 받아 답변(response)을 생성하는 함수\n",
    "    response = {  # 반환할 결과를 저장할 빈 딕셔너리 초기화\n",
    "        \"standalone_query\": \"\",  # 검색용 최종 쿼리 저장\n",
    "        \"topk\": [],              # 검색된 문서들의 docid 리스트 저장\n",
    "        \"references\": [],        # 검색된 문서들의 score와 내용 저장\n",
    "        \"answer\": \"\"             # 최종 생성된 답변 텍스트 저장\n",
    "    }\n",
    "\n",
    "    # ✅ 1차 호출 준비 (질문이 과학 관련인지 판단하기 위한 system 메시지 추가)\n",
    "    msg = [{\"role\": \"system\", \"content\": persona_function_calling}] + messages  # 시스템 페르소나 + 사용자 메시지 합치기\n",
    "    try:\n",
    "        result_json = call_solar(msg, model=llm_model, temperature=0.0)  # Solar API 호출 (결과를 JSON으로 받음)\n",
    "    except Exception:\n",
    "        traceback.print_exc()  # 오류 발생 시 에러 트레이스 출력\n",
    "        response[\"answer\"] = \"답변을 생성하는 도중 오류가 발생했습니다.\"  # 오류 메시지 세팅\n",
    "        return response  # 빈 결과 반환\n",
    "\n",
    "    # ✅ 1차 호출 결과에서 생성된 assistant 메시지 가져오기\n",
    "    message = result_json[\"choices\"][0][\"message\"]  # Solar 결과에서 첫 번째 응답 메시지 꺼내기\n",
    "    tool_calls = message.get(\"tool_calls\", None)    # tool_calls (함수 호출 여부) 확인\n",
    "\n",
    "    # ✅ 검색이 필요한 경우 (tool_calls 존재)\n",
    "    if tool_calls:\n",
    "        tool_call = tool_calls[0]  # 첫 번째 tool_call 사용\n",
    "        args = json.loads(tool_call[\"function\"][\"arguments\"])  # tool_call 안의 arguments를 dict로 변환\n",
    "        query = args.get(\"standalone_query\")  # standalone_query (검색 쿼리) 추출\n",
    "\n",
    "        search_result = sparse_retrieve(query, 3)  # sparse(BM25) 검색 실행하여 관련 문서 3개 가져오기\n",
    "        response[\"standalone_query\"] = query  # 검색 쿼리 기록\n",
    "\n",
    "        context = []  # 검색된 문서 본문 리스트 초기화\n",
    "        for hit in search_result['hits']['hits']:  # 검색 결과 문서들을 순회\n",
    "            context.append(hit[\"_source\"][\"content\"])  # 문서 내용 추가\n",
    "            response[\"topk\"].append(hit[\"_source\"][\"docid\"])  # 문서 docid 추가\n",
    "            response[\"references\"].append({  # 문서 score와 내용 추가\n",
    "                \"score\": hit[\"_score\"],\n",
    "                \"content\": hit[\"_source\"][\"content\"]\n",
    "            })\n",
    "\n",
    "        # ✅ 검색 결과를 assistant 메시지로 추가\n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": json.dumps(context)  # 검색 문서들을 JSON 문자열로 변환\n",
    "        })\n",
    "\n",
    "        # ✅ 2차 호출 준비 (검색 문서를 참고해서 최종 답변 생성)\n",
    "        qa_msg = [{\"role\": \"system\", \"content\": persona_qa}] + messages  # 시스템 페르소나(답변 생성용) + 메시지 합치기\n",
    "\n",
    "        try:\n",
    "            qa_result_json = call_solar(qa_msg, model=llm_model, temperature=0.0)  # Solar API 다시 호출 (최종 답변용)\n",
    "        except Exception:\n",
    "            traceback.print_exc()  # 오류 발생 시 에러 트레이스 출력\n",
    "            return response  # 빈 결과 반환\n",
    "\n",
    "        response[\"answer\"] = qa_result_json[\"choices\"][0][\"message\"][\"content\"]  # 최종 생성된 답변을 response에 저장\n",
    "    else:\n",
    "        # ✅ 검색 없이 바로 답변하는 경우\n",
    "        response[\"answer\"] = message[\"content\"]  # 1차 호출 결과를 바로 답변으로 사용\n",
    "\n",
    "    return response  # 최종 생성된 response 반환\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe78f6d",
   "metadata": {},
   "source": [
    "## 평가 루프 \n",
    "### OPENAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e4614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def eval_rag(eval_filename, output_filename):\n",
    "    with jsonlines.open(eval_filename) as reader, open(output_filename, \"w\") as of:\n",
    "        for idx, j in enumerate(reader):\n",
    "            print(f'Test {idx}\\nQuestion: {j[\"msg\"]}')\n",
    "            \n",
    "            try:\n",
    "                resp = answer_question(j[\"msg\"])  # Solar API를 사용하는 answer_question 호출\n",
    "            except Exception as e:\n",
    "                print(f\"Error during answering: {e}\")\n",
    "                resp = {\"standalone_query\": \"\", \"topk\": [], \"references\": [], \"answer\": \"답변 생성 실패\"}\n",
    "\n",
    "            print(f'Answer: {resp[\"answer\"]}\\n')\n",
    "\n",
    "            out = {\n",
    "                \"eval_id\": j[\"eval_id\"],\n",
    "                \"standalone_query\": resp[\"standalone_query\"],\n",
    "                \"topk\": resp[\"topk\"],\n",
    "                \"answer\": resp[\"answer\"],\n",
    "                \"references\": resp[\"references\"]\n",
    "            }\n",
    "            of.write(json.dumps(out, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            time.sleep(0.5)  # ✅ 요청 간 딜레이 추가\n",
    "\n",
    "eval_rag(\"./data/eval1.jsonl\", \"sample_submission2.csv\")  # 위에서 정의한 eval_rag 함수를 호출하여 평가를 시작한다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
